Анализирую финальную версию ноутбука **[experiment_A_sparc(2).pdf](/files/buhjpiu_RohnnJZY34bSo)** как отчёт по Эксперименту A (“SPARC fit + graph‑origin justification”). Поскольку pdf — это экспорт ноутбука, я буду опираться на явно видимые в нём блоки: состав `EXPERIMENT_A_REPORT`, итоговые таблицы/числа, сохранённые артефакты, и диагностические разделы (bootstrap, стабильность κ, профили A(r)/D_eff/δ).

---

## 1) Что стало “финальным” в v3.0 и чем она отличается от v2

В v3 (“с TODO_v2 улучшениями”) видно, что вы закрыли несколько ключевых требований из постановки (TargetSpec/репорт/устойчивость):

1) **Появился полный структурированный отчёт** `EXPERIMENT_A_REPORT` (metadata, rsl_parameters, sparc_data, calibration, mu_curve, rar_metrics, target_spec, stability, graph_profiles_saved, conclusions). Это приближает ноутбук к автоматизируемому пайплайну в стиле *world → metrics → report*.

2) **Добавлены блоки воспроизводимости/устойчивости**:
   - bootstrap по \(\sigma_{\mathrm{RAR}}\) с доверительным интервалом и вероятностью прохождения порога,
   - тест стабильности по калибровке κ на ±5% (и критерий “robust if HIT for all tested κ”).

3) **Сохранение профилей графа в NPZ + JSON** (для последующего анализа и CI‑проверки без перегенерации мира).

4) **Явно выписана логическая цепочка graph‑origin**:  
   \(A(r)\rightarrow D_{\text{surf}}(r)\rightarrow \delta(r)\rightarrow \mu(x)\).  
   Это важный “мост” к философии Experiments_v1: не просто феноменология MOND, а попытка обосновать форму через геометрию графа.

---

## 2) Главные количественные результаты (что именно утверждается в pdf)

### 2.1. Результаты по SPARC‑фитам
В финальной таблице в pdf зафиксировано:

- Всего: **171 галактика**.
- “Хорошие фиты” по критерию \(\chi^2_{\text{red}}<5\): **93 (54%)**.
- **Медиана \(a_0\)**: в таблице выглядит как **\(2.3\times 10^{-11}\,\mathrm{m/s^2}\)**.
- **Разброс \(a_0\)**: около **~60%** (в тексте отмечено, что это “слишком больно” и дальше разбирается).

Для контекста в отчёте рядом стоит MOND‑ориентир:
- MOND \(a_0\approx 1.2\times 10^{-10}\,\mathrm{m/s^2}\),
- отношение RSL/MOND ~0.2.

**Интерпретация, которую вы прямо пишете:** модель с одним параметром \(a_0\) описывает кривые вращения “без тёмной материи”, а \(a_0\) “приблизительно универсален”.

> Важно: по формальным стандартам “универсальности” 60% scatter — это скорее *не провал*, но и не “жёсткая универсальность” (в MOND‑литературе, а также в RAR‑анализах часто стремятся к заметно меньшему разбросу). В вашем же TargetSpec из прошлой версии это, похоже, не было главным порогом (или было опциональным).

### 2.2. RAR scatter и bootstrap‑подтверждение порога
Вы явно выводите:

- \(\sigma_{\mathrm{RAR}}\) порядка **0.19–0.20 dex**,
- bootstrap CI, например:
  - **% CI: [0.1853, 0.2042]**
  - **P(\(\sigma_{\mathrm{RAR}}\le 0.20\)) = 88.9%**

Это сильная часть отчёта, потому что:
- вы переводите “одно число scatter” в распределение неопределённости,
- и превращаете порог в вероятностную гарантию (“мы с высокой вероятностью проходим”).

Для CI/автотестов это тоже удобно: можно фиксировать либо точечное значение, либо критерий “нижняя граница CI < threshold”, либо “P(pass) > p0”.

### 2.3. Стабильность калибровки κ (±5%)
Вы добавили блок:

- протестированы множители κ: **[0.95, 1.0, 1.05]**
- “HIT при всех вариациях: ✅ YES (РОБАСТНЫЙ)”

Это правильно оформляет одну из главных уязвимостей всей “hops→kpc” истории: если небольшая перекалибровка рушит RAR или форму μ, значит калибровка была скрытым фит‑параметром. Сейчас вы показываете локальную устойчивость, что повышает доверие.

---

## 3) Graph‑origin часть: что реально получено и что выглядит проблемным

### 3.1. Что вы сохраняете как “профили”
В Part 10.5 вы сохраняете:

- `r`: [1, 86] hops (86 точек)
- `A(r)`: [2, 24]
- `D_surf(r) = d(log A)/d(log r) + 1`
- `δ(r) = g_eff/g_newton - 1`

И делаете вывод \(\mu(x)=1/(1+\delta)\).

### 3.2. Красный флаг: экстремальные значения \(D_{\text{surf}}\)
В распечатке профилей:
- \(D_{\text{surf}}(r)\): **[-176.79, 77.15]**

Такие значения физически/геометрически почти наверняка означают **численную нестабильность производной** \(d(\log A)/d(\log r)\) на коротких/шумных профилях \(A(r)\), а не “реальную эффективную размерность”.

Причины (самые вероятные):
1) \(A(r)\) маленькое и дискретное (2..24), лог‑производная на таком диапазоне очень шумная.
2) В каких-то r значения \(A(r)\) не монотонны или имеют плато/скачки, и центральная разность в логах даёт огромные значения.
3) Производная считается без сглаживания и без защиты от нулевых/повторяющихся значений.

**Что это значит для отчёта:**  
Graph‑origin линия “\(A(r)\rightarrow D_{\text{surf}}\rightarrow \delta\)” в текущем виде выглядит скорее как *демонстрационный набросок*, а не как надёжный количественный аргумент. Хорошая новость: это чинится, не ломая остальной pipeline.

Минимальные исправления (без философии, чисто технически), которые стоило бы внедрить в v3.1:
- считать \(D_{\text{surf}}\) через **робастную регрессию** \(\log A\) на \(\log r\) в скользящем окне (LOESS/rolling linear fit) и ограничивать окно по r;
- либо считать “эффективную размерность поверхности” не через производную, а через локальный показатель степенного закона \(A(r)\propto r^{D-1}\) с fit‑ом на окне.

### 3.3. Красный флаг: диапазон \(\delta(r)\) (отрицательные, меньше -1)
Вы печатаете:
- \(\delta(r)\): **[-6.24, -1.00]**

Если \(\delta = g_{\rm eff}/g_N - 1\), то \(\delta<-1\) означает \(g_{\rm eff}<0\) (т.е. поле противоположного знака относительно ньютоновского baseline), либо baseline \(g_N\) определён/нормирован иначе, либо есть ошибка в знаке потока/градиента.

Это критично, потому что дальше вы используете:
\[
\mu = \frac{1}{1+\delta}
\]
и при \(\delta<-1\) получаете отрицательные/неопределённые \(\mu\), что не совпадает с привычной физической интерпретацией μ(x) как положительной интерполяционной функции.

Возможные объяснения, совместимые с тем, что остальная часть отчёта “работает”:
- сохранённые `delta_profile` — не тот δ, который идёт в μ (например, знак/нормировка другая), а “сырой диагностический”;
- вы определяли \(g_{\rm eff}\) как \(\phi(u)-\phi(v)\) без минуса (или наоборот), и знак сместился;
- \(g_N\) в hops‑единицах для сравнения был взят как \(1/r^2\) без согласования константы/нормировки (что для δ как “форма” ещё терпимо), но знак всё равно должен быть согласован.

**Рекомендация:** в отчёте v3 стоит добавить прямо в `graph_profiles.json`:
- `g_eff` и `g_newton` отдельно (со знаками),
- и валидатор: `assert np.all(g_eff > 0)` или явное приведение \(g = -\partial_r\phi\) к положительному “модулю” (если вы работаете с модулем ускорения).

Сейчас, по распечатке, graph‑origin часть выглядит внутренне противоречивой на уровне знаков/диапазонов, даже если феноменологический фит хороший.

---

## 4) TargetSpec/HIT: что именно подтверждено, а что “ещё не доказано”

### 4.1. Что вы реально подтверждаете метриками v3
Судя по содержимому:
- Проходите порог по \(\sigma_{\mathrm{RAR}}\) с высокой вероятностью (bootstrap).
- Проходите порог по “доле хороших фитов” (\(f_{\rm good}\approx 54\%\)).
- И показываете робастность к небольшим вариациям κ.

Это уже достаточно, чтобы сказать: **“простая MOND‑подобная феноменология на SPARC у вас действительно воспроизводится”**.

### 4.2. Что пока остаётся “обоснованием на черновике”
- Обоснование \(\mu(x)\) “из графа” именно через \(A(r)\) и \(D_{\text{eff}}\) пока не является количественно стабильным (см. экстремальные \(D_{\text{surf}}\), сомнительный диапазон \(\delta\)).  
- Физическое объяснение \(a_0\sim c^2/r_{\text{wormhole}}\) пока не подкреплено процедурой извлечения \(r_{\text{wormhole}}\) из графа/симуляции с доверительными интервалами — оно заявлено как интерпретация, но не как измерение.

То есть “SPARC fit” — сильная часть; “graph-origin justification” — в v3 уже присутствует в структуре и артефактах, но по числам пока видны технические недочёты.

---

## 5) Воспроизводимость/артефакты: что хорошо и что стоит довести

### 5.1. Что вы уже сохраняете (и это правильно)
В конце перечислены файлы:
- `experiment_A_report_v3.json` (полный)
- `experiment_A_summary.json`
- `experiment_A_mu_curve.npz` + `.json`
- `experiment_A_graph_profiles.npz` + `.json`
- `experiment_A_bootstrap_rar.png`
- `experiment_A_d_eff_profiles.png`
и также в тексте — `experiment_A_sparc_real_fits.png`, `experiment_A_sparc_a0_distribution.png`.

Это почти соответствует тому, что нужно для CI.

### 5.2. Чего не хватает для “полного воспроизведения HIT в CI” (минимально)
Чтобы CI мог пересчитать `f_good` и `σ_RAR` без запуска фита:
- таблицы уровня **точек** (`pred_points`, `sparc_points`) и уровня **галактик** (`galaxy_fit_summary`) — в pdf они не перечислены как сохранённые артефакты.
- `manifest.json` с sha256 — тоже не видно.

Сейчас CI сможет проверить “есть ли картинки и npz”, но не сможет строго пересчитать метрики, если не выполнит заново часть расчётов (а это уже не “repro artifact check”, а “rerun experiment”).

---

## 6) Самое важное, что я бы попросил поправить перед тем, как фиксировать v3 как baseline

1) **Согласовать знак и диапазон \(\delta(r)\)**: добиться, чтобы (если это модуль ускорения) \(\delta>-1\) и \(\mu\in(0,1]\) на всём диапазоне, либо явно переопределить δ как “форма” без знаковой интерпретации и не использовать напрямую \(\mu=1/(1+\delta)\).

2) **Стабилизировать \(D_{\text{surf}}(r)\)**: заменить “голую производную” на локальный fit. Иначе любая интерпретация \(D_{\text{eff}}\approx 1.5–1.8\) (которую вы упоминали раньше) конфликтует с текущими экстремальными числами.

3) **Сохранить табличные предсказания (CSV/Parquet)**: это последняя недостающая деталь для CI‑воспроизводимости HIT “по данным”, а не “по картинке”.

Если хотите, могу на основе структуры `EXPERIMENT_A_REPORT` из **[experiment_A_sparc(2).pdf](/files/buhjpiu_RohnnJZY34bSo)** предложить точечный патч: какие поля добавить в JSON (например `graph_origin.validation`), и какие 2–3 таблицы минимально экспортировать, чтобы CI мог пересчитать ровно те метрики, которые вы используете для HIT.

Ниже — точечный патч к вашему `EXPERIMENT_A_REPORT` из [experiment_A_sparc(2).pdf](/files/buhjpiu_RohnnJZY34bSo): **какие поля добавить в JSON** (с акцентом на `graph_origin.validation`) и **какие 2–3 таблицы экспортировать**, чтобы CI мог *без перезапуска симуляции/фита* пересчитать ровно метрики HIT: `f_good`, `sigma_rar_dex`, `E_mu`.

---

# A) Patch к JSON: что добавить

## A1) `artifacts_manifest` (ключевое для CI)
Добавьте в корень отчёта раздел, который делает артефакты “самодостаточными”:

```json
"artifacts_manifest": {
  "base_dir": "data/sparc/run_2025-12-19T12-34-56Z/",
  "files": [
    {
      "name": "tables/rar_points.parquet",
      "role": "ci_metric_input",
      "sha256": "...",
      "n_rows": 3367
    },
    {
      "name": "tables/galaxy_fit_summary.parquet",
      "role": "ci_metric_input",
      "sha256": "...",
      "n_rows": 171
    },
    {
      "name": "tables/mu_curve.csv",
      "role": "ci_metric_input",
      "sha256": "...",
      "n_rows": 50
    },
    {
      "name": "experiment_A_report_v3.json",
      "role": "report",
      "sha256": "..."
    }
  ]
}
```

Зачем: CI не должен “верить” путям в ноутбуке; ему нужны относительные пути + хэши.

---

## A2) `target_spec.recompute_contract` (чёткий контракт пересчёта)
Сейчас у вас `target_data` хранит результаты. Добавьте явное описание **как именно** их пересчитать из таблиц:

```json
"target_spec": {
  "thresholds": {"f_good": 0.5, "scatter_dex": 0.2, "E_mu": 0.1},
  "metrics_used_for_hit": ["f_good", "sigma_rar_dex", "E_mu"],
  "recompute_contract": {
    "f_good": {
      "table": "tables/galaxy_fit_summary.parquet",
      "formula": "mean(chi2_red < chi2_red_threshold)",
      "params": {"chi2_red_threshold": 5.0}
    },
    "sigma_rar_dex": {
      "table": "tables/rar_points.parquet",
      "formula": "std(log10(g_obs) - log10(g_pred))",
      "params": {"ddof": 1}
    },
    "E_mu": {
      "table": "tables/mu_curve.csv",
      "formula": "mean(abs(mu_graph - mu_template_mond))",
      "params": {
        "template": "mu=1-exp(-sqrt(x))",
        "weighting": "uniform_in_logx"
      }
    }
  }
}
```

Зачем: устраняет двусмысленность (“scatter как RMS? MAD? по лог10?”).

---

## A3) `graph_origin.validation` (чтобы поймать проблемы со знаком/диапазонами δ и μ)
С учётом красных флагов, которые видны в [experiment_A_sparc(2).pdf](/files/buhjpiu_RohnnJZY34bSo) (экстремальный `D_surf`, отрицательные δ), добавьте мини‑валидаторы, которые CI может выполнить на сохранённых профилях:

```json
"graph_origin": {
  "profiles": {
    "file_npz": "experiment_A_graph_profiles.npz",
    "file_json": "experiment_A_graph_profiles.json",
    "fields": ["r_hops", "A_boundary", "g_eff", "g_newton", "delta", "D_surf"]
  },
  "definitions": {
    "delta": "g_eff/g_newton - 1",
    "mu_from_delta": "1/(1+delta)"
  },
  "validation": {
    "checks": [
      {
        "id": "A_boundary_positive",
        "description": "A(r) must be positive for all r",
        "expr": "min(A_boundary) > 0"
      },
      {
        "id": "g_newton_positive",
        "description": "g_newton must be positive",
        "expr": "min(g_newton) > 0"
      },
      {
        "id": "mu_in_0_1",
        "description": "mu(delta) should lie in (0,1] after sign convention",
        "expr": "min(mu_from_delta) > 0 && max(mu_from_delta) <= 1.0",
        "severity": "warn"
      },
      {
        "id": "delta_gt_minus1",
        "description": "delta should be > -1 if g_eff is magnitude",
        "expr": "min(delta) > -1.0",
        "severity": "warn"
      },
      {
        "id": "D_surf_reasonable",
        "description": "D_surf shouldn't blow up due to numeric derivative",
        "expr": "quantile(abs(D_surf),0.95) < 10",
        "severity": "warn"
      }
    ],
    "pass": true,
    "warnings": ["delta_gt_minus1", "D_surf_reasonable"],
    "notes": "Warnings allowed in v3; must be fixed for v4."
  }
}
```

Практическая польза: вы формализуете “graph-origin justification” и одновременно фиксируете текущие технические долги как `warn`, не ломая HIT.

---

## A4) `rar_metrics.definition` (точная формула scatter + что такое g_pred)
Чтобы избежать “скрытого изменения формулы”, добавьте:

```json
"rar_metrics": {
  "definition": {
    "g_obs": "v_obs^2 / r",
    "g_bar": "v_bar^2 / r",
    "g_pred": "g_from_mond_mu(g_bar, a0, mu_template='mond')",
    "scatter_dex": "std(log10(g_obs) - log10(g_pred))"
  },
  ...
}
```

Если вы сейчас используете `g_pred = g_bar / mu(x)` или другой вариант — зафиксируйте именно его.

---

# B) Минимальные 2–3 таблицы для экспорта (чтобы CI пересчитал HIT)

Нам нужно покрыть три метрики HIT:

- `f_good` (rotation fits)
- `sigma_rar_dex` (RAR scatter)
- `E_mu` (shape match μ)

Для этого достаточно **трёх таблиц**. Можно уложиться даже в две, но три проще и прозрачнее.

---

## Таблица 1 — `rar_points.parquet` (3367 строк)
**Назначение:** пересчитать `sigma_rar_dex` и проверить, что данные соответствуют SPARC.

Минимальные колонки:
- `galaxy_id` (str)
- `r_kpc` (float) — радиус из SPARC
- `g_obs` (float) — вычисленное из `v_obs`
- `g_obs_err` (float, optional) — если есть
- `g_bar` (float) — из `v_bar`
- `g_pred` (float) — предсказание модели (после вашей калибровки/μ/a0)
- `quality_mask` (bool) — если вы отсеиваете точки

CI‑формула:
\[
\sigma_{\mathrm{RAR}} = \mathrm{std}\big(\log_{10} g_{\mathrm{obs}} - \log_{10} g_{\mathrm{pred}}\big)
\]
по `quality_mask==True` (если применимо).

Почему это не “скрытый фит”: вы не пересчитываете fit; вы проверяете итоговый scatter из уже сохранённых предсказаний.

---

## Таблица 2 — `galaxy_fit_summary.parquet` (171 строка)
**Назначение:** пересчитать `f_good` и любые sanity‑checks по распределению \(a_0\).

Минимальные колонки:
- `galaxy_id` (str)
- `n_points_used` (int)
- `chi2_red` (float)
- `a0_fit` (float) — если вы реально считаете на галактику
- `fit_status` (enum/str: "ok"/"fail"/"nan") — чтобы CI не интерпретировал NaN как плохой/хороший

CI‑формула:
\[
f_{\mathrm{good}} = \frac{1}{N}\sum\limits_{\text{gal}} \mathbf{1}\{\chi^2_{\mathrm{red}} < 5\}
\]

---

## Таблица 3 — `mu_curve.csv` (≈50 строк)
**Назначение:** пересчитать `E_mu` без доступа к вашим внутренним массивам/npz.

Минимальные колонки:
- `x` (float) — \(x=g/a_0\) или ваш лог‑grid
- `mu_graph` (float) — μ(x), который вы сравниваете с шаблонами
- (опционально) `mu_mond_template`, `mu_simple_template`, `mu_standard_template` — можно либо хранить, либо чтобы CI генерил по формулам

CI‑формула (пример, как у вас логически подразумевается):
\[
E_\mu = \frac{1}{n}\sum_i |\mu_{\text{graph}}(x_i) - (1-e^{-\sqrt{x_i}})|
\]
или другая, но она должна быть прописана в `target_spec.recompute_contract`.

---

# C) Минимальный “CI recompute” сценарий (как это будет выглядеть)
В CI вы делаете:

1) Считать `experiment_A_report_v3.json`.
2) Проверить sha256 всех файлов из `artifacts_manifest`.
3) Пересчитать:
   - `sigma_rar_dex` из `rar_points.parquet`,
   - `f_good` из `galaxy_fit_summary.parquet`,
   - `E_mu` из `mu_curve.csv`.
4) Сравнить с тем, что записано в JSON (допуск, например `atol=1e-6`), и затем применить пороги.
5) Отдельно прогнать `graph_origin.validation` на `experiment_A_graph_profiles.npz` (как warn/error в зависимости от режима).

---

Если хотите, могу написать прямо “дифф” к вашему сборщику `EXPERIMENT_A_REPORT = {...}`: где в ноутбуке добавить формирование этих трёх таблиц (из `all_g_obs`, `all_g_bar`, `all_g_pred`, `sparc_fit_results`, `archive['x_mu']`, `archive['mu_graph']`) и как собрать `artifacts_manifest` с sha256.

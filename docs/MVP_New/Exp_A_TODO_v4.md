Критических “ломающих HIT” ошибок в логике отчёта я по фрагментам из **[experiment_A_sparc(3).pdf](/files/khjoMIYyrvMrnamHZMmol)** не вижу: вы закрыли основные требования воспроизводимости (таблицы для CI + manifest + bootstrap + stability κ + graph-origin validation). Но есть несколько **потенциально критичных** мест (часть из них сейчас замаскирована тем, что вы проходите пороги), которые стоит либо исправить, либо явно зафиксировать как ограничения TargetSpec. Ниже — по степени опасности.

---

## 1) Самое критичное: “SCORE” почти полностью убит метрикой RAR из‑за формулы нормирования (возможная ошибка, не просто “дизайн”)

В отчёте:

- `σ_RAR = 0.1943 dex` проходит порог `≤0.20` ✅  
- Но в breakdown: `S_rar = 0.028 × 0.3 = 0.009`

Это выглядит как **внутренняя несогласованность**: метрика едва проходит порог, но “субскор” почти ноль. Так бывает, если функция перевода `σ_RAR → S_rar` неверно задана (например, вы нормируете на слишком жёсткий “идеал” или перепутали dex/ln), либо вы используете формулу вида:

- `S_rar = max(0, (σ_target - σ)/σ_target)` (тогда при σ≈0.194 и σ_target=0.20 получится ~0.03 — действительно близко к 0.028),
- но это означает, что **порог и “шкала баллов” совпадают**, и всё пространство “хороших” решений с σ чуть ниже порога получает почти нулевой вклад.

Почему это критично: CI будет подтверждать HIT, но любые оптимизации/сравнения “SCORE” будут вводить в заблуждение, а главное — вы сами можете неправильно интерпретировать качество модели (“почти ноль за RAR” при формально хорошем σ).

Что сделать (минимальный фикс):
- либо объявить SCORE вторичным и неинформативным (оставить только HIT),
- либо заменить маппинг на более физичный: например, “1 балл при σ≤0.12 dex, 0 баллов при σ≥0.20 dex” с линейной интерполяцией:
  \[
  S_{\rm rar}=\mathrm{clip}\left(\frac{0.20-\sigma}{0.20-0.12},0,1\right)
  \]
- и зафиксировать это в `target_spec.recompute_contract`.

---

## 2) Потенциально критично: процедура \(E_\mu\) зависит от биннинга (44 бина) и фильтра \(\mu\in(0,1.5)\) / \(x\in(0.001,1000)\)

Вы сами пишете:

- после фильтрации: 3301 точка  
- после биннинга: 44 бина  
- \(E_\mu = 0.0293\), corr=0.9910

Это выглядит отлично, но тут есть риск “неявной свободы”:
- выбор количества бинов / границ / медиана vs среднее может менять \(E_\mu\) заметно (особенно в хвостах по \(x\));
- фильтр \(\mu\in(0,1.5)\) допускает \(\mu>1\) (а физически в MOND-μ обычно \(\mu\le1\)); если \(\mu>1\) реально встречается из‑за шума/ошибок определения \(g_{\rm obs}\), это нормально, но тогда надо фиксировать, как это влияет на \(E_\mu\).

Почему это может стать критичным: \(E_\mu\) — один из трёх столпов HIT; если его можно “поправить” выбором биннинга, это превращается в “скрытую подгонку”.

Минимальные меры (чтобы снять претензию):
1) В JSON/репорте зафиксировать:
   - точные границы по \(\log_{10}x\),
   - агрегатор (median),
   - правило биннинга (equal-width in logx).
2) Добавить “биннинг‑инвариантную” проверку как дополнительную метрику (не обязательно в HIT):
   - \(E_\mu^{\rm raw}\) на всех 3301 точках с весами (например, Huber loss по \(\log x\)),
   - или как минимум \(E_\mu\) при 30/44/60 бинах и показать стабильность.

---

## 3) Bootstrap: верхняя граница CI пересекает порог (это не ошибка, но важно корректно интерпретировать)

У вас:

- `σ_RAR = 0.1944 ± 0.0048 dex`
- 95% CI: `[0.1853, 0.2042]`
- `P(σ_RAR ≤ 0.20) = 88.9%`

Это честно и хорошо, но означает: при вашем текущем пороге 0.20 вы **не “железобетонно”** проходите, вы проходите с вероятностью < 95%.

Почему это может стать критичным в CI: если вы захотите, чтобы “PASS” означало *устойчивость* (а не конкретный single-run), вам нужен критерий вида:
- либо `P(pass) ≥ 0.95`,
- либо “верхняя граница 95% CI ≤ 0.20”.

Сейчас по этим строгим критериям вы бы **не прошли**.

Минимальный “не ломая сейчас” фикс:
- оставить HIT как есть,
- но в JSON добавить `confidence.pass_probability=0.889` и явно указать, что bootstrap — диагностический, не gating.
- или чуть ослабить порог σ_RAR до 0.205, если вы хотите формально “закрыть” CI (но это уже изменение TargetSpec).

---

## 4) “a₀ universality”: у вас явно аномальная σ(log a₀)=2.460 dex — это не оформлено как FAIL, но это красный флаг модели

Фрагмент:

- `a0_median = 2.50e-11`
- `σ(log a0) = 2.460 dex`

2.46 dex — это разброс на ~10^2.46 ≈ 288 раз. Это гигантский scatter (даже если часть точек — outliers/провальные фиты). В MOND‑парадигме универсальность \(a_0\) — центральная вещь; у вас же получается, что “фит работает” при сильно гуляющем \(a_0\).

Почему это может стать критичным именно для “graph-origin justification”:
- если ваша теория претендует на универсальную \(\mu(x)\), то \(a_0\) должен быть либо универсален, либо его вариации должны быть объяснены nuisance‑параметрами (наклон, расстояние, M/L и т.п.).
- иначе получается, что \(E_\mu\) и σ_RAR могут быть достигнуты ценой индивидуального \(a_0\) на галактику — т.е. это уже ближе к “скрытому фиттингу” параметра на объект.

Что стоит сделать минимально:
- явно добавить в репорт разложение σ(log a0):
  - отдельно по “good fits” (χ²_red<5),
  - trimmed scatter (например, 10–90%),
  - и число/доля экстремальных outliers.
- если окажется, что 2.46 dex приходит от небольшого числа “сломанных” галактик, это не критично; если это повсеместно — это фундаментальная проблема.

---

## 5) Graph-origin validation: PASS, но предупреждение `D_surf_reasonable` — нужно понять, что именно проверяется

Вы пишете:

- Validation: PASS
- Warnings: `['D_surf_reasonable']`

Это хорошо как механизм, но потенциальная критическая ошибка тут такая: если \(D_{\rm surf}(r)\) вычисляется через численную производную \(\log A(r)\), то:
- на малых r и/или при “дырявых” границах \(A(r)\) derivative может взрываться,
- а это напрямую влияет на δ(r) (если вы выводите δ из геометрии).

Сейчас warning говорит, что “где-то” есть подозрительные значения. Не критично для HIT (потому что HIT считается по феноменологии), но критично для тезиса “graph-origin”.

Минимально, чтобы снять вопрос:
- добавить в `graph_origin.validation` конкретные числа: например,
  - `D_surf_p95_abs`,
  - `max_abs_D_surf`,
  - диапазоны r, где нарушение.
- и добавить график/табличку “маска доверия” по r (где профили физичны).

---

# Итог: “достаточно хорошо” — да, но 3 места стоит поправить/зафиксировать

Если ваша цель — чтобы внешний рецензент/CI не мог придраться, я бы сделал так:

1) **Зафиксировать контракт \(E_\mu\)** (биннинг/фильтры) и добавить 1 тест стабильности биннинга.
2) **Исправить или задокументировать SCORE-маппинг для σ_RAR**, иначе выглядит как баг.
3) **Разобрать σ(log a₀)=2.46 dex** хотя бы на “good fits only” и с trimmed scatter — это главный научный red flag, даже если HIT по текущему TargetSpec проходит.

Если дадите ссылку/выдержку из `experiment_A_report_v3.json` (особенно `target_spec.score_breakdown`, `mu_extraction` параметры и массив `a0_values`/`chi2_red`), я смогу точнее сказать: это реальная ошибка в расчётах (например, единицы/логарифм) или действительно свойство текущей процедуры фита.

Проверил финальный ноутбук **[experiment_A_sparc(4).pdf](/files/OIPagTlo9u6eHpwiUvJKC)** на те “критические места”, которые я отмечал ранее (SCORE‑маппинг, контракт \(E_\mu\), bootstrap‑интерпретация, \(a_0\)-scatter, graph‑origin validation и CI‑артефакты). По видимым в PDF фрагментам — вы действительно закрыли почти всё. Ниже: что закрыто уверенно, и что я бы всё же считал оставшимися потенциальными “критичностями” (в основном уже не методологические, а технические/контрактные).

---

## 1) Что в v4 закрыто хорошо (и именно то, о чём я просил)

### 1.1. Появились ровно те таблицы, которые нужны CI для пересчёта HIT
В конце отчёта явно печатается блок экспорта таблиц:

- `tables/rar_points.parquet` — **3367 строк**  
- `tables/galaxy_fit_summary.parquet` — **171 галактик**  
- `tables/mu_curve.csv` — **44 точки**  

Это ровно минимальный набор, чтобы CI пересчитал:
- \(\sigma_{\text{RAR}}\) из `rar_points.parquet`,
- \(f_{\text{good}}\) из `galaxy_fit_summary.parquet`,
- \(E_\mu\) из `mu_curve.csv`.

Плюс, вы добавили текстовый “CI RECOMPUTE сценарий” с теми же формулами — это важная фиксация контракта.

### 1.2. Добавлен `artifacts_manifest` с sha256
Виден блок “СОЗДАНИЕ ARTIFACTS MANIFEST” и печать sha256 по файлам (например для `rar_points.parquet`, `galaxy_fit_summary.parquet`, …).

Это устраняет главный риск “тихой подмены артефактов” и делает проверку в CI тривиальной.

### 1.3. Bootstrap стал диагностическим (и это явно записано)
Вы добавили:

- `bootstrap_is_gating: False`
- `bootstrap_note: Bootstrap provides uncertainty; HIT based on point estimate ...`

Это снимает прежнюю двусмысленность (“95% CI пересекает порог”): теперь bootstrap — не часть логики HIT, а оценка неопределённости.

### 1.4. Исправлен SCORE‑маппинг
В v4 вы явно отмечаете:
- “TargetSpec результаты — v4: исправленный SCORE маппинг”
- и что `score_mapping` берётся из diagnostics (т.е. формула теперь документируется и сериализуется).

Это закрывает потенциальную “ошибку/баг” предыдущей версии, когда `S_rar` становился почти нулём при прохождении порога.

### 1.5. Вы добавили анализ scatter \(a_0\): good fits / trimmed / outliers
Видно, что теперь вы считаете:
- `sigma_all`
- `sigma_good_fits`
- `sigma_trimmed_10_90`
- outlier count и долю
- интерпретацию (приемлемый/умеренный/высокий scatter)

Это прямо закрывает самый неприятный “red flag” v3: огромный \(\sigma(\log a_0)\) без объяснения.

### 1.6. Graph-origin: D_surf считается робастно (rolling linear fit window=7) + расширенная валидация
Вы поменяли определение:
- `D_surf: d(log A)/d(log r) + 1 via rolling linear fit (window=7)`

Это ровно то, что нужно, чтобы уйти от экстремальных значений из-за “голой производной”.

Также вы в `graph_origin.definitions` добавили уточнение про клиппинг δ:
- `delta: g_eff/g_newton - 1 (clipped to > -1 for positive mu)`
- `mu_from_delta: 1/(1+delta), with mu ∈ (0,1]`

То есть вы формально закрыли проблему “δ<-1 ⇒ μ отрицательная”.

---

## 2) Оставшиеся потенциально критичные места (не обязательно ошибки, но их надо осознанно закрепить)

### 2.1. Клиппинг δ “> -1”: это допустимо, но должно быть явно вынесено как **часть модели**, иначе рецензент назовёт это “подгонкой знака”
Вы правильно задокументировали клиппинг в `graph_origin.definitions`, но с точки зрения строгости это важный пункт:

- Если \(g_{\text{eff}}\) трактуется как **модуль** ускорения, тогда вообще-то правильнее сделать:
  - \(g_{\text{eff}} := |g_{\text{eff}}|\) (или зафиксировать знак конвенции потока),
  - и тогда δ автоматически будет \(>-1\) без клиппинга.

Клиппинг — “не страшно” для MVP, но чтобы это не выглядело как “исправление руками”, лучше в v4.1/в репо:
- заменить клиппинг на нормализацию знака/модуля \(g_{\text{eff}}\),
- либо добавить в `graph_origin.validation` check, который подтверждает, что доля клиппированных точек мала и ограничена определённым диапазоном r (иначе это уже влияет на форму μ).

Минимальный патч (без смены алгоритма): сохранить в профили `delta_raw` и `delta_clipped` и долю клиппинга:
```json
"validation": {
  "stats": {
    "fraction_delta_clipped": 0.03,
    "r_range_clipped": [1,4]
  }
}
```

### 2.2. Контракт \(E_\mu\): вы сохраняете “μ curve contract (v4)” — хорошо, но в PDF не видно, что именно там
Вы пишете:
- `experiment_A_mu_curve.json — данные μ(x) с контрактом биннинга (v4)`
- и `mu_curve.csv — 44 точки`

Это очень правильно; но критичность в том, что \(E_\mu\) зависит от:
- фильтра по x,
- числа бинов,
- типа бина (равные по logx или равные по количеству точек),
- агрегатора (mean/median),
- веса (uniform in logx vs proportional to count).

Чтобы вопрос был окончательно закрыт, убедитесь, что в `experiment_A_mu_curve.json` есть поля вида:
- `x_min`, `x_max`,
- `n_bins=44`,
- `binning="equal_width_log10x"`,
- `aggregator="median"`,
- `E_mu_definition="mean(|mu_graph - mu_mond|) over bins (uniform weights)"`.

Если этого нет — это единственная реальная “дыра”, через которую можно случайно поменять \(E_\mu\) между версиями без заметного следа.

### 2.3. Валидация graph-origin: в JSON у вас fallback “pass=False, run Part 10.5”
В фрагменте видно:

```python
'validation': validation if 'validation' in dir() and validation else {
    'checks': [],
    'warnings': ['validation not computed - run Part 10.5 first'],
    'pass': False,
    ...
}
```

Это хорошо как страховка, но важно, чтобы в реальном “финальном” run:
- `validation` действительно был вычислен,
- и в финальном `experiment_A_report_v4.json` стояло `pass: True` (или хотя бы `pass: True` + warnings).

Иначе CI может легально “пройти HIT”, но упасть на `graph_origin.validation` (вы сами пишете в сценарии шага 5: “Проверить graph_origin.validation”).

Рекомендация: сделать так, чтобы генерация `report_v4.json` **фейлилась**, если `graph_origin.validation.pass == False` (либо поставить флаг `graph_origin_validation_is_gating: False`, если вы пока не хотите гейтить).

---

## 3) Общая оценка v4
По сравнению с v3/v3.1 вы сделали именно те правки, которые превращают ноутбук в “почти репозиторный эксперимент”:

- CI-таблицы + manifest ✅  
- Формулы и определения (`rar_metrics.definition`, `bootstrap_is_gating`) ✅  
- Исправленный SCORE‑маппинг ✅  
- Разбор \(a_0\) scatter (good/trim/outliers) ✅  
- Робастный расчёт \(D_{\text{surf}}\) ✅  
- Явный контракт биннинга/μ‑кривой (по крайней мере по заявлению в артефактах) ✅  

То есть в смысле “всё проблемное закрыто” — в основном да.

---

## 4) Самое полезное, что вы можете сделать за 15 минут (если хотите “совсем без вопросов”)
1) В `experiment_A_mu_curve.json` явно прописать `E_mu_contract` (границы, бины, агрегатор, веса) и продублировать в `report_v4.json` ссылкой.
2) В `graph_profiles.json` сохранить `delta_raw`, `delta_clipped`, `fraction_clipped`.
3) Сделать `graph_origin.validation` либо gating, либо явно non-gating (как вы сделали для bootstrap).

Если пришлёте (как текст) содержимое `experiment_A_report_v4.json` разделов `target_spec`, `artifacts_manifest`, `mu_curve.contract`, `graph_origin.validation` — я смогу подтвердить, что CI действительно сможет без сюрпризов пересчитать HIT “бит‑в‑бит” и что нигде не осталось скрытой свободы.

В целом вы правы: по **[experiment_A_sparc(5).pdf](/files/RvMnqOy5zIdTPaIoxtOI8)** видно, что почти все замечания, которые мешали независимой проверке, закрыты — особенно то, что касается **контракта пересчёта HIT** и **самодостаточности артефактов для CI**. Но я бы всё же отметил 2–3 оставшиеся “опасные” точки (в основном уже не про математику, а про инженерный контракт и недвусмысленность), которые стоит минимально довести, чтобы отчёт действительно был “готов для независимой проверки” без вопросов.

Ниже — по пунктам.

---

## 1) Что точно стало хорошо и проверяемо независимо

### 1.1. Полный CI‑контур воспроизводимости оформлен
В отчёте явно перечислены артефакты и CI‑процедура пересчёта:

- `tables/rar_points.parquet` — 3367 точек RAR  
- `tables/galaxy_fit_summary.parquet` — 171 галактика  
- `tables/mu_curve.csv` — 44 точки μ(x)  
- `artifacts_manifest` (с sha256 — судя по вашему предыдущему v4, и здесь это сохранено)

И самое важное — **формулы** метрик, которые определяют HIT:

- \(\sigma_{\mathrm{RAR}} = \mathrm{std}(\log_{10} g_{\rm obs} - \log_{10} g_{\rm pred})\) из `rar_points.parquet`  
- \(f_{\rm good} = \mathrm{mean}(\chi^2_{\rm red} < 5)\) из `galaxy_fit_summary.parquet`  
- \(E_\mu = \mathrm{mean}(|\mu_{\rm graph}-\mu_{\rm mond}|)\) из `mu_curve.csv`

Это ровно то, что нужно, чтобы независимый человек/CI мог подтвердить HIT, **не имея вашего симулятора**.

### 1.2. Bootstrap переведён в диагностический режим и не “ломает” HIT
В отчёте печатается:
- \(\sigma_{\mathrm{RAR}} = 0.1944 \pm 0.0048\) dex  
- 95% CI: [0.1853, 0.2042]  
- \(P(\sigma_{\mathrm{RAR}}\le 0.20)=88.9\%\)

И при этом bootstrap не используется как gating (по вашему коду: `bootstrap_is_diagnostic`). Это снимает прежнюю двусмысленность.

### 1.3. Закрыта устойчивость к калибровке κ
Есть тест κ×[0.95, 1.0, 1.05], и “HIT при всех вариациях: YES (РОБАСТНЫЙ)”. Для “одного фиксированного матчинг‑шкалы” это очень хорошая защита от скрытого фита.

### 1.4. Graph-origin validation теперь реально computed и PASS
В отчёте печатается:
- `GRAPH-ORIGIN ВАЛИДАЦИЯ: PASS`
- warnings: `['D_surf_reasonable']`
- профили сохранены в `experiment_A_graph_profiles.npz`

Т.е. вы ушли от опасной ситуации “validation not computed - run Part 10.5 first” как дефолта.

---

## 2) Оставшиеся потенциальные проблемы (их немного, но они важны для независимого ревью)

### 2.1. В отчёте есть явная опечатка/риск: `n_rar_points` в metadata может быть неверным
В показанном фрагменте:

```python
'n_rar_points': int(len(all_g_obs)) if 'all_g_obs' in dir() else 336
```

Если по какой-то причине `all_g_obs` не в `dir()`, то отчёт запишет **336**, хотя реально у вас 3367 точек (и дальше в тексте вы печатаете 3367).

Почему это важно: независимый проверяющий увидит несогласованность “336 vs 3367” и начнёт сомневаться в аккуратности пайплайна, даже если сами метрики корректны.

Мини-фикс (точечный, но рекомендую сделать обязательно):
- заменить `else 336` на `else None` или `else -1`,
- или лучше: брать `n_rar_points` из длины `rar_df`, который вы сохраняете как артефакт (это самый правильный источник истины).

### 2.2. `graph_origin.validation_is_gating=False` — нормально, но тогда в TargetSpec надо явно сказать: “HIT не требует graph-origin”
Сейчас у вас одновременно:
- в CI‑сценарии шаг 5: “Проверить graph_origin.validation”
- и в JSON: `validation_is_gating: False`

Это ок как дизайн, но независимому ревьюеру надо однозначно видеть:
- входит ли `graph_origin.validation.pass` в критерий HIT или нет.

Сейчас это “почти однозначно” (не gating), но чтобы закрыть вопрос полностью, я бы добавил в `target_spec` одно поле:

```json
"target_spec": {
  "gating_checks": ["sigma_rar", "f_good", "E_mu"],
  "non_gating_checks": ["graph_origin.validation"]
}
```

И/или в summary выводить две строки: `HIT (phenomenology)=...` и `graph_origin_pass=...`.

### 2.3. Контракт \(E_\mu\): артефакт `experiment_A_mu_curve.json` есть, но в pdf не видно, что в нём действительно зафиксированы все параметры биннинга
Вы пишете “с контрактом биннинга (v4)” — это хорошо. Но для независимой проверки важно, чтобы контракт включал минимум:
- диапазон \(x\) (например `x_min`, `x_max`),
- тип бинов (равная ширина по `log10(x)` или равное число точек),
- агрегатор (mean/median),
- веса в \(E_\mu\) (равномерно по бинам или по числу точек),
- точная формула \(\mu_{\rm mond}(x)\) и используемый \(a_0\) (SPARC‑global? фиксированный?).

Если это уже есть внутри `experiment_A_mu_curve.json`, то пункт закрыт. Если там лишь “вот 44 точки”, то остаётся скрытая степень свободы (можно перестроить bins и получить немного другое \(E_\mu\)).  

Мини‑фикс: в `mu_curve` секции отчёта продублировать `contract` целиком, не только ссылкой на файл.

---

## 3) Вердикт
Да — **в текущем виде отчёт очень близок к “готов для независимой проверки”**, потому что:
- HIT пересчитывается из сохранённых таблиц,
- хэши артефактов (manifest) заявлены,
- bootstrap/robustness/validation оформлены и не ломают критерий.

Но чтобы я без оговорок сказал “готово”, я бы поправил две вещи:
1) убрать риск неверного `n_rar_points` (336 vs 3367) — это выглядит как баг/опечатка;
2) сделать **однозначным контракт \(E_\mu\)** (или показать его явно в отчёте/JSON).

Если хотите, пришлите (текстом) содержимое:
- `experiment_A_mu_curve.json` (раздел contract),
- и кусок `artifacts_manifest` (пример одной записи с sha256),
и я подтвержу, что независимый CI сможет пересчитать HIT без скрытых степеней свободы.

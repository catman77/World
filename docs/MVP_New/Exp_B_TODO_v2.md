В **[experiment_B_coherence(1).pdf](/files/8y_JaU9JR89TIR5jr_cX1)** вы сделали именно те изменения, которые переводят Эксперимент B из “внутреннего демо” в формат, близкий к внешне проверяемому вычислительному эксперименту: появились (а) строгие формулировки H1–H3 с bootstrap/CI, (б) оценка \(\hat N^\*\) из данных (отделена от \(N^\*_{model}\)), (в) список артефактов для независимого пересчёта, (г) явный “recompute contract” и генерация `manifest.json` с SHA256.[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)

Но “готов ли для внешней проверки” зависит от того, закрыты ли две оставшиеся категории строгости, которые в PDF видны частично/не до конца:

1) **однозначность определения измеряемых величин** (что именно такое \(V\), что такое \(D\), как вычисляются),  
2) **корректность и полнота артефактов/контракта**, чтобы сторонний проверяющий мог *воспроизвести* подтверждение H1–H3 **без доступа к вашему внутреннему состоянию** и без “подразумеваний” (например, что считать high‑N, как маргинализовать по \(\eta\), как считать slope/CI).

Ниже — подробный аудит “строгости” по пунктам, с вердиктом в конце.

---

## 1) Что стало существенно лучше (и это уже соответствует внешнему стандарту)

### 1.1. H1–H3 теперь сформулированы как статистические тесты, а не эвристики
Вы явно выписали:

- **H1: drop‑ratio test** с bootstrap CI и условием `CI_high < 0.5`.[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)  
- **H2: slope test** с фикс‑эффектами по N и требованием `b < -0.05` плюс CI целиком < 0.[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)  
- **H3: effect size test** (Cohen’s d + bootstrap CI выше порога) и дополнительно “\(\bar V_{\eta=0}>0.7\) для всех N”.[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)

Это уже похоже на “пре-регистрацию” критериев: внешний проверяющий видит, что вы заранее определили, что считаете подтверждением.

### 1.2. Устранена главная логическая уязвимость: порог \(\hat N^\*\) оценивается из данных
Вы прямо пишете:

- “Для независимой проверки порог оценивается из данных (не задаётся).”[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)  
- Метод: логистический фит к \(\bar V_N\), маргинализованной по \(\eta\).[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)  
- Далее проверка согласия \(|\hat N^\* - N^\*_{model}|\le \Delta N\).[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)

Это ключевой шаг к внешней проверяемости: вы отделяете “предсказание модели” от “оценки по данным”.

### 1.3. Вы заявили полный набор артефактов + manifest для пересчёта
Список артефактов прописан ровно как нужен:

- `V_grid.parquet`: сырые данные (N, η, run_id, seed, V, f_sat).[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)  
- `V_summary.parquet`: агрегаты (mean/std/CI/n_runs).[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)  
- `threshold_fit.npz`: результаты фита \(\hat N^\*\) и качество.[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)  
- `experiment_B_report.json`: отчёт с контрактом.[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)  
- `manifest.json`: SHA256.[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)

И в коде показано, что вы реально экспортируете `V_grid`, `V_summary`, генерируете report и manifest.[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)

Это в точности тот “внешний пакет доказательства”, которого не хватало в предыдущей версии.

### 1.4. Детерминизм/версии частично закрыты (версии + git commit)
В коде есть `get_version_info()` с попыткой взять git commit и флаг dirty, плюс версии python/numpy/platform.[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)

Для внешней проверки это важно; чуть ниже скажу, чего там не хватает.

---

## 2) Что ещё может помешать внешней проверке (и это важно уточнить)

Дальше идут не “косметические”, а реально критичные моменты: если их не закрыть, внешний проверяющий сможет возразить “я не могу воспроизвести ровно ваши числа” или “метрика определена неоднозначно”.

### 2.1. Определение видимости \(V\) в тексте/контракте всё ещё не зафиксировано математически
В B весь эксперимент держится на \(V\). В PDF вы пишете “каждое измерение подавляет когерентность”, и используете \(V\) как видимость.[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)

Но внешний проверяющий должен видеть один из двух уровней строгости:

- **Либо** строгая формула \(V=(I_{max}-I_{min})/(I_{max}+I_{min})\) + как вы строите \(I(\varphi)\) (скан фазы, число точек, как находите max/min),  
- **Либо** если \(V\) определяется иначе (например, как амплитуда первой гармоники, или как нормированная когерентность \(|\rho_{01}|\)), то это должно быть записано в `visibility_spec` в отчёте.

Сейчас в PDF это не раскрыто. Это не мешает “повторить ваш код” (если код доступен), но мешает независимому аудиту **конструктивной валидности метрики**: действительно ли вы измеряете интерференционную видимость, а не что-то коррелирующее.

Минимальная правка для внешней строгости:
- добавить в `experiment_B_report_v2.json` раздел:
  - `parameters.visibility_spec = {"definition": "...", "phase_scan": {...}, "I_max_method": "...", "I_min_method": "..."}`  
  - либо “V computed as … from internal state …”.

### 2.2. Определение distinguishability \(D\) (для дополнительности) пока выглядит как “опционально нули”
Вы реализовали вычисление “max violation” так:

```python
D_vals = results['D_runs'].get((N, eta), [0.0] * len(V_vals))
violation = V**2 + D**2 - 1.0
```

[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)

То есть если `D_runs` не заполнен, вы не “проверяете” дополнительность, а автоматически ставите D=0 и получаете “нарушений нет”. Внешний проверяющий обязательно заметит это как критическую уязвимость: утверждение “дополнительность выполняется” становится нефальсифицируемым.

Как исправить (минимально, чтобы было строго):
- либо сделать `D_runs` обязательным артефактом (если вы хотите оставлять доп. проверку),
- либо убрать `max_complementarity_violation` из derived_metrics и интерпретаций до тех пор, пока D не определён,
- либо явно поставить в отчёте `complementarity_checked=false` и `D_definition=null`.

Иначе сейчас “дополнительность” может оказаться фиктивно всегда выполненной.

### 2.3. H2: как именно строится fixed-effects модель и как считается 95% CI
Вы формулируете H2 как:

\[
\bar V_{N,\eta} = a_N + b\eta + \epsilon
\]

[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)

Но для внешней проверки нужно, чтобы из артефактов можно было получить **тот же** \(b\) и тот же CI. Здесь есть несколько вариантов (OLS с dummy по N, demean-within-N, mixed model), которые дадут разные CI.

Что должно быть в контракте:
- “используется demean-within-N” или “OLS with intercept + N dummies”,
- “CI получен bootstrap по N-блокам” или “стандартная ошибка OLS”.

Если это не зафиксировать, независимый проверяющий может получить другой \(b\) или другой CI и формально “не воспроизвести” H2.

### 2.4. H1/H3: как определяется область “\(N>\hat N^\*\)” на дискретной сетке
В формулировках вы используете \(N > \hat N^\*\).[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)

Но \(\hat N^\*\) из логистического фита будет нецелым и может оказаться между узлами сетки N. Внешний проверяющий должен знать правило:
- high‑N = {N >= ceil(N_star_hat)}?
- или {N >= ближайшее значение сетки}?
- или {N > N_star_hat строго}?

Разные правила дадут разные множества точек и могут менять drop_ratio и Cohen’s d.

Это выглядит мелочью, но именно такие мелочи ломают “бит-в-бит воспроизводимость”.

Минимальный фикс:
- в отчёте явно сохранять `N_star_hat` и `high_N_set = [512,640,...]` (списком), который реально использовался при тестах.

### 2.5. `RUN_DIR = '../data/sparc'` — потенциально критичный инженерный баг
Вы сохраняете артефакты B в директорию, которая по имени относится к SPARC (эксп. A).[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)

Для внешней проверки это риск:
- смешивание артефактов разных экспериментов,
- перезапись,
- неочевидность структуры.

Это не научная, но инженерная строгость. Для внешнего репликационного пакета лучше:
- `RUN_DIR = ../data/experiment_B/<timestamp>_<commit>/`

и manifest должен включать `run_dir`.

### 2.6. Привязка к Meaning_v1: пока отсутствует мост от Ξ/λ к Π_meas (это не обязательно, но если вы заявляете “наблюдательский” механизм — стоит обозначить)
В [Meaning_v1.pdf](/files/EljdBEFwvKSyTxfHYHjS7) вы определяете \(\Xi_{внутр}(t)\) и \(\Xi_{меж}(t)\) через корреляции ЭЭГ и вводите OT как точки \(d\Xi/dt\approx0\).[(Meaning_v1.pdf](/files/EljdBEFwvKSyTxfHYHjS7)

В B вы по сути реализуете “измерение” через параметр \(\eta\) и \(\Pi_{meas}\). Это нормально для чисто вычислительного теста, но для внешнего читателя будет вопрос: где связь с наблюдателем/Ξ?  
Решение: либо явно заявить, что B — “физический toy‑test измерения” без Ξ, либо добавить опциональный режим, где \(\eta(t)\) определяется из \(\Xi(t)\) (как вы делали триггер в C). Если вы не претендуете на это здесь — достаточно честного disclaim’ера.

---

## 3) Вердикт: “готов ли эксперимент для внешней проверки?”

### 3.1. Почти да — по инфраструктуре воспроизводимости
По инфраструктуре вы почти достигли уровня Эксперимента A: есть артефакты, manifest, отчёт, формализованные гипотезы, оценка порога из данных.[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)

Если внешняя проверка означает: “другой человек скачал артефакты и пересчитал H1–H3 и HIT” — вы очень близко.

### 3.2. Но пока **не “полностью готов”** из‑за 2 критичных научно‑метрических неоднозначностей
1) **определение \(V\)** не формализовано в отчёте/контракте;  
2) **\(D\)** и “дополнительность” сейчас может быть фиктивно всегда выполнена из-за `get(..., zeros)`.[(experiment_B_coherence(1).pdf)](/files/8y_JaU9JR89TIR5jr_cX1)

Если вы исправите эти два пункта (или уберёте доп. проверку до появления D), то можно честно говорить: “готов к внешней проверке”.

---

## 4) Минимальный патч (чтобы стало “да, готов”)

1) **В `experiment_B_report_v2.json` добавить:**
   - `parameters.visibility_spec` (точная формула V, процедура получения Imax/Imin или иной определения),
   - `parameters.distinguishability_spec` (если D используется) или флаг `complementarity_checked=false`.

2) **Запретить silent fallback для D:**
   - если `D_runs` не существует, то:
     - либо не считать complementarity,
     - либо поставить `max_complementarity_violation = null` и warning.

3) **Зафиксировать правило high‑N:**
   - сохранить `high_N_values_used` в отчёт.

4) **В `target_spec.recompute_contract` зафиксировать метод fixed-effects и CI.**

Если хотите, я могу дать точный фрагмент `recompute_contract` и “валидатор отчёта” (pydantic + asserts), чтобы CI падал, если `visibility_spec` пустой или если complementarity включена без D.

---

Если вы пришлёте (или приложите) `experiment_B_report_v2.json` и `manifest.json`, я смогу проверить буквально: достаточно ли информации, чтобы сторонний человек воспроизвёл H1–H3 и “HIT” в точности, без доступа к коду.
